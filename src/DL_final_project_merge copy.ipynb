{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nR3YdnsQTtOj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import reduce\n",
        "from operator import __add__\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "        self.kernel_sizes = (3, 3)\n",
        "        self.conv_padding = reduce(__add__, \n",
        "            [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in self.kernel_sizes[::-1]])\n",
        "\n",
        "    def forward(self, input):\n",
        "        residual = input\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(input)\n",
        "        x = self.bn1(self.conv1(x))\n",
        "        x = self.prelu(x)\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(input)\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += residual\n",
        "        return x\n",
        "\n",
        "class NoiseGenerator(nn.Module):\n",
        "    def __init__(self, block, num_of_resblock=5, input_channels=3):\n",
        "        super(NoiseGenerator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 3, kernel_size=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(3)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.layers = self._make_layer(block, num_of_resblock)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(input_channels, 3, kernel_size=3, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(3)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(input_channels, 3, kernel_size=3, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(3)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.kernel_sizes = (3, 3)\n",
        "        self.conv_padding = reduce(__add__, \n",
        "            [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in self.kernel_sizes[::-1]])\n",
        "\n",
        "    def _make_layer(self, block, num_of_resblock):\n",
        "        layers = []\n",
        "        for i in range(num_of_resblock):\n",
        "          layers.append(block(in_channels=3, out_channels=3))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(input)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.layers(x)\n",
        "\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        comb = x + residual\n",
        "\n",
        "        comb = nn.ZeroPad2d(self.conv_padding)(comb)\n",
        "        comb = self.conv3(comb)\n",
        "        comb = self.bn3(comb)\n",
        "        comb = self.relu3(comb)\n",
        "\n",
        "        return comb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6TvN2vsWXXX",
        "outputId": "ec8a77a7-0951-41a9-a72b-a73570c5295b"
      },
      "outputs": [],
      "source": [
        "# model = NoiseGenerator(ResBlock)\n",
        "# print(model)\n",
        "# input = torch.ones([4,3,28,28])\n",
        "# output = model(input)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRdg-0gRYAgp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import reduce\n",
        "from operator import __add__\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, stride=2):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=stride, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.bn(self.conv(input))\n",
        "        # print(x.shape)\n",
        "        x = self.lrelu(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, block, input_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 3, kernel_size=3, padding='same', bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(3)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.channels = [64, 128, 128, 256, 256, 512, 512]\n",
        "        self.strides = [2, 1, 2, 1, 2, 1, 2]\n",
        "        self.layers = self._make_layer(block, self.channels, self.strides)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.dense = nn.LazyLinear (out_features=1024)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.dense2 = nn.Linear(1024, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, chennels_num, strides):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels=3, out_channels=chennels_num[0], stride = strides[0]))\n",
        "        for i in range(1,len(chennels_num)-1):\n",
        "          layers.append(block(in_channels=chennels_num[i-1], out_channels=chennels_num[i], stride = strides[i]))\n",
        "        layers.append(block(in_channels=chennels_num[-2], out_channels=chennels_num[-1], stride = strides[-1]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x = self.layers(x)\n",
        "        print(x.shape)\n",
        "        x = self.flat(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.sig(x)\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9pb6IdWg777",
        "outputId": "92a77238-8bf5-4727-eaee-a61d747719a7"
      },
      "outputs": [],
      "source": [
        "# model = Discriminator(ConvLayer)\n",
        "# print(model)\n",
        "# input = torch.ones([4,3,64,64])\n",
        "# output = model(input)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import ddp\n",
        "from ddp import Unet\n",
        "from data import DatasetLoader\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# LOAD MODEL\n",
        "ddp.MODEL.load_state_dict(torch.load(\"unet_final.pt\"))\n",
        "ddp.MODEL.to(device)\n",
        "BATCH = ddp.BATCH_SIZE\n",
        "NormalDataloader = DatasetLoader(\"../data/dataset_same_size/train/good\", batch=BATCH, size=ddp.IMG_SIZE)\n",
        "AnomalyDataloader = DatasetLoader(\"../data/dataset_same_size/train/defect\", batch=BATCH, size=ddp.IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eTMhFf1ag_Fy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/re6111032/miniconda3/envs/dlfinal/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "/home/re6111032/DL_2023_spring/final/src/ddp.py:296: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  image=sample_timestep(torch.tensor(image,device=device),t)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3, 256, 256])\n",
            "torch.Size([8, 512, 13, 13])\n",
            "torch.Size([8, 3, 256, 256])\n",
            "torch.Size([8, 512, 13, 13])\n"
          ]
        }
      ],
      "source": [
        "# Instantiate generator and discriminator\n",
        "generator = NoiseGenerator(ResBlock)\n",
        "discriminator = Discriminator(ConvLayer)\n",
        "forward = ddp.forward\n",
        "reverse = ddp.denoise\n",
        "\n",
        "# Specify loss functions\n",
        "gan_loss = nn.BCELoss()\n",
        "aux_loss = nn.MSELoss()  # Change to the appropriate loss function for your task\n",
        "\n",
        "# Set up optimizers\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for Anomaly, Normal in zip(AnomalyDataloader, NormalDataloader): #normal 不夠的話請用data augmentation補充到跟所有anomaly相同的數量(或是刪掉幾張anomaly)\n",
        "\n",
        "        # ============= setting ==========\n",
        "        batch_size = Anomaly.size(0)\n",
        "\n",
        "        # Generate fake images\n",
        "        generated_noise = generator(Anomaly)\n",
        "        Noised_anomaly = generated_noise + Anomaly\n",
        "\n",
        "        # Generate noised normal images\n",
        "        Noised_normal = forward(Normal, batch=BATCH)\n",
        "\n",
        "        # Generate Denoised images\n",
        "        Denoised_normal = reverse(Noised_normal)\n",
        "        Denoised_anomaly = reverse(Noised_anomaly)\n",
        "\n",
        "\n",
        "        # =========== Training ==========\n",
        "        # Train the discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        \n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "        \n",
        "        real_outputs = discriminator(Noised_anomaly)\n",
        "        fake_outputs = discriminator(Noised_normal)\n",
        "        \n",
        "        discriminator_loss = gan_loss(real_outputs, real_labels) + gan_loss(fake_outputs, fake_labels)\n",
        "        discriminator_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "        \n",
        "        # Train the generator\n",
        "        generator_optimizer.zero_grad()\n",
        "        \n",
        "        fake_outputs = discriminator(Noised_anomaly)\n",
        "        \n",
        "        generator_loss = gan_loss(fake_outputs, real_labels)\n",
        "        ## generator_loss.backward()\n",
        "        ## generator_optimizer.step()\n",
        "        \n",
        "        # Compute auxiliary loss\n",
        "        auxiliary_loss = aux_loss(Denoised_normal, Denoised_anomaly)  # Modify according to your task\n",
        "        \n",
        "        # Update generator parameters again with auxiliary loss\n",
        "        ## generator_optimizer.zero_grad()\n",
        "        (generator_loss + auxiliary_loss).backward()\n",
        "        generator_optimizer.step()\n",
        "        \n",
        "        # Print losses or other metrics\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Generator Loss: {generator_loss.item()}, \"\n",
        "              f\"Discriminator Loss: {discriminator_loss.item()}, Auxiliary Loss: {auxiliary_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
