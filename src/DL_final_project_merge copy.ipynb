{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nR3YdnsQTtOj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\acvlab\\anaconda3\\envs\\hwf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import reduce\n",
        "from operator import __add__\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "        self.kernel_sizes = (3, 3)\n",
        "        self.conv_padding = reduce(__add__, \n",
        "            [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in self.kernel_sizes[::-1]])\n",
        "\n",
        "    def forward(self, input):\n",
        "        residual = input\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(input)\n",
        "        x = self.bn1(self.conv1(x))\n",
        "        x = self.prelu(x)\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(input)\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += residual\n",
        "        return x\n",
        "\n",
        "class NoiseGenerator(nn.Module):\n",
        "    def __init__(self, block, num_of_resblock=5, input_channels=3):\n",
        "        super(NoiseGenerator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 3, kernel_size=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(3)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.layers = self._make_layer(block, num_of_resblock)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(input_channels, 3, kernel_size=3, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(3)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(input_channels, 3, kernel_size=3, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(3)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.kernel_sizes = (3, 3)\n",
        "        self.conv_padding = reduce(__add__, \n",
        "            [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in self.kernel_sizes[::-1]])\n",
        "\n",
        "    def _make_layer(self, block, num_of_resblock):\n",
        "        layers = []\n",
        "        for i in range(num_of_resblock):\n",
        "          layers.append(block(in_channels=3, out_channels=3))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(input)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.layers(x)\n",
        "\n",
        "        x = nn.ZeroPad2d(self.conv_padding)(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        comb = x + residual\n",
        "\n",
        "        comb = nn.ZeroPad2d(self.conv_padding)(comb)\n",
        "        comb = self.conv3(comb)\n",
        "        comb = self.bn3(comb)\n",
        "        comb = self.relu3(comb)\n",
        "\n",
        "        return comb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6TvN2vsWXXX",
        "outputId": "ec8a77a7-0951-41a9-a72b-a73570c5295b"
      },
      "outputs": [],
      "source": [
        "# model = NoiseGenerator(ResBlock)\n",
        "# print(model)\n",
        "# input = torch.ones([4,3,28,28])\n",
        "# output = model(input)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRdg-0gRYAgp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import reduce\n",
        "from operator import __add__\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, stride=2):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=stride, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.bn(self.conv(input))\n",
        "        # print(x.shape)\n",
        "        x = self.lrelu(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, block, input_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, 3, kernel_size=3, padding='same', bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(3)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.channels = [64, 128, 128, 256, 256, 512, 512]\n",
        "        self.strides = [2, 1, 2, 1, 2, 1, 2]\n",
        "        self.layers = self._make_layer(block, self.channels, self.strides)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.dense = nn.LazyLinear (out_features=1024)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.dense2 = nn.Linear(1024, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, chennels_num, strides):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels=3, out_channels=chennels_num[0], stride = strides[0]))\n",
        "        for i in range(1,len(chennels_num)-1):\n",
        "          layers.append(block(in_channels=chennels_num[i-1], out_channels=chennels_num[i], stride = strides[i]))\n",
        "        layers.append(block(in_channels=chennels_num[-2], out_channels=chennels_num[-1], stride = strides[-1]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x = self.layers(x)\n",
        "        print(x.shape)\n",
        "        x = self.flat(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.sig(x)\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9pb6IdWg777",
        "outputId": "92a77238-8bf5-4727-eaee-a61d747719a7"
      },
      "outputs": [],
      "source": [
        "# model = Discriminator(ConvLayer)\n",
        "# print(model)\n",
        "# input = torch.ones([4,3,64,64])\n",
        "# output = model(input)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import ddp\n",
        "from ddp import Unet\n",
        "from data import DatasetLoader\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# LOAD MODEL\n",
        "ddp.MODEL.load_state_dict(torch.load(\"unet_final.pt\"))\n",
        "ddp.MODEL.to(device)\n",
        "ddp.BATCH_SIZE=16\n",
        "NormalDataloader = DatasetLoader(\"./data/dataset_same_size/train/good\",batch=ddp.BATCH_SIZE,size=ddp.IMG_SIZE)\n",
        "AnomalyDataloader = DatasetLoader(\"./data/dataset_same_size/train/defect\",batch=ddp.BATCH_SIZE,size=ddp.IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eTMhFf1ag_Fy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\acvlab\\anaconda3\\envs\\hwf\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 256, 256])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\acvlab\\Desktop\\ddpm_gan\\ddp.py:296: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  image=sample_timestep(torch.tensor(image,device=device),t)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 1.3984006643295288, Auxiliary Loss: 13.25069808959961\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 13.942922592163086\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 13.453810691833496\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 12.783763885498047\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 7.4152750968933105\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 13.958036422729492\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.578332901000977\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 6.359907150268555\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 9.107616424560547\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 11.524740219116211\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 12.202146530151367\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 8.43197250366211\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.154937744140625\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 12.597186088562012\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 6.874948501586914\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 15.140922546386719\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.167745590209961\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 8.543639183044434\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.605754852294922\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 11.409839630126953\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 13.172563552856445\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 11.15431022644043\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 11.068119049072266\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.924410820007324\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 7.500479698181152\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 15.4512939453125\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.670507431030273\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 10.078411102294922\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "torch.Size([16, 3, 256, 256])\n",
            "torch.Size([16, 512, 13, 13])\n",
            "Epoch [1/5], Generator Loss: 100.0, Discriminator Loss: 100.0, Auxiliary Loss: 8.452278137207031\n",
            "torch.Size([16, 3, 256, 256])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m Noised_normal \u001b[39m=\u001b[39m forward(Normal, batch\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Generate Denoised images\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m Denoised_normal \u001b[39m=\u001b[39m reverse(Noised_normal)\n\u001b[0;32m     33\u001b[0m Denoised_anomaly \u001b[39m=\u001b[39m reverse(Noised_anomaly)\n\u001b[0;32m     36\u001b[0m \u001b[39m# =========== Training ==========\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Train the discriminator\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\acvlab\\Desktop\\ddpm_gan\\ddp.py:296\u001b[0m, in \u001b[0;36mdenoise\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,T)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m    295\u001b[0m         t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m,), i, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m--> 296\u001b[0m         image\u001b[39m=\u001b[39msample_timestep(torch\u001b[39m.\u001b[39;49mtensor(image,device\u001b[39m=\u001b[39;49mdevice),t)\n\u001b[0;32m    297\u001b[0m         \u001b[39m#show_tensor_image(image.detach().to('cpu'))\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \n\u001b[0;32m    299\u001b[0m     \u001b[39m#t1=show_tensor_image(image.detach().to('cpu'))\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
            "File \u001b[1;32mc:\\Users\\acvlab\\anaconda3\\envs\\hwf\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\acvlab\\Desktop\\ddpm_gan\\ddp.py:174\u001b[0m, in \u001b[0;36msample_timestep\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m# Call model (current image - noise prediction)\u001b[39;00m\n\u001b[0;32m    171\u001b[0m model_mean \u001b[39m=\u001b[39m sqrt_recip_alphas_t \u001b[39m*\u001b[39m (\n\u001b[0;32m    172\u001b[0m     x \u001b[39m-\u001b[39m betas_t \u001b[39m*\u001b[39m MODEL(x, t) \u001b[39m/\u001b[39m sqrt_one_minus_alphas_cumprod_t\n\u001b[0;32m    173\u001b[0m )\n\u001b[1;32m--> 174\u001b[0m posterior_variance_t \u001b[39m=\u001b[39m get_index_from_list(posterior_variance, t, x\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mreturn\u001b[39;00m model_mean\n",
            "File \u001b[1;32mc:\\Users\\acvlab\\Desktop\\ddpm_gan\\ddp.py:32\u001b[0m, in \u001b[0;36mget_index_from_list\u001b[1;34m(vals, t, x_shape)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mReturns values from vals for corresponding timesteps\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mwhile considering the batch dimension.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m batch_size \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m output \u001b[39m=\u001b[39m vals\u001b[39m.\u001b[39mgather(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, t\u001b[39m.\u001b[39;49mcpu())\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m*\u001b[39m((\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(x_shape) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39mto(t\u001b[39m.\u001b[39mdevice)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Instantiate generator and discriminator\n",
        "generator = NoiseGenerator(ResBlock)\n",
        "discriminator = Discriminator(ConvLayer)\n",
        "forward = ddp.forward\n",
        "reverse = ddp.denoise\n",
        "\n",
        "# Specify loss functions\n",
        "gan_loss = nn.BCELoss()\n",
        "aux_loss = nn.MSELoss()  # Change to the appropriate loss function for your task\n",
        "\n",
        "# Set up optimizers\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0005)\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for Anomaly, Normal in zip(AnomalyDataloader, NormalDataloader): #normal 不夠的話請用data augmentation補充到跟所有anomaly相同的數量(或是刪掉幾張anomaly)\n",
        "        print(Anomaly.shape)\n",
        "        # ============= setting ==========\n",
        "        batch_size = Anomaly.size(0)\n",
        "\n",
        "        # Generate fake images\n",
        "        generated_noise = generator(Anomaly)\n",
        "        Noised_anomaly = generated_noise + Anomaly\n",
        "\n",
        "        # Generate noised normal images\n",
        "        #Noised_normal = forward(Normal, batch=ddp.BATCH_SIZE)\n",
        "        Noised_normal = forward(Normal, batch=batch_size)\n",
        "\n",
        "        # Generate Denoised images\n",
        "        Denoised_normal = reverse(Noised_normal)\n",
        "        Denoised_anomaly = reverse(Noised_anomaly)\n",
        "\n",
        "\n",
        "        # =========== Training ==========\n",
        "        # Train the discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        \n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "        \n",
        "        real_outputs = discriminator(Noised_anomaly)\n",
        "        fake_outputs = discriminator(Noised_normal)\n",
        "        \n",
        "        discriminator_loss = gan_loss(real_outputs, real_labels) + gan_loss(fake_outputs, fake_labels)\n",
        "        discriminator_loss.backward(retain_graph=True)\n",
        "        discriminator_optimizer.step()\n",
        "        \n",
        "        # Train the generator\n",
        "        generator_optimizer.zero_grad()\n",
        "        \n",
        "        fake_outputs = discriminator(Noised_anomaly)\n",
        "        \n",
        "        generator_loss = gan_loss(fake_outputs, real_labels)\n",
        "        ## generator_loss.backward()\n",
        "        ## generator_optimizer.step()\n",
        "        \n",
        "        # Compute auxiliary loss\n",
        "        auxiliary_loss = aux_loss(Denoised_normal, Denoised_anomaly)  # Modify according to your task\n",
        "        \n",
        "        # Update generator parameters again with auxiliary loss\n",
        "        ## generator_optimizer.zero_grad()\n",
        "        (generator_loss + auxiliary_loss).backward(retain_graph=True)\n",
        "        generator_optimizer.step()\n",
        "        \n",
        "        # Print losses or other metrics\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Generator Loss: {generator_loss.item()}, \"\n",
        "              f\"Discriminator Loss: {discriminator_loss.item()}, Auxiliary Loss: {auxiliary_loss.item()}\")\n",
        "        \n",
        "    torch.save(generator.state_dict(), \"generator_%d.pt\" %(epoch) )    \n",
        "    torch.save(discriminator.state_dict(), \"discrimina_%d.pt\" %(epoch) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
